{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\CSANADANSYS\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from transformers import DistilBertForQuestionAnswering, TrainingArguments, Trainer, DefaultDataCollator, BitsAndBytesConfig\n",
    "from peft import LoraConfig, PeftModel\n",
    "import os\n",
    "import wandb\n",
    "from trl import SFTTrainer\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import collections\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading and preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "# The MRQA dataset is included in huggingface's datasets library, so we just have to load it\n",
    "# Loading dataset (smaller fraction than in the final becasue had to train on local GPU)\n",
    "mrqa = load_dataset(\"mrqa\", split=\"train[:5%]\")\n",
    "# Creating the train-test-validation split\n",
    "mrqa = mrqa.train_test_split(test_size=0.2)\n",
    "mrqa[\"train\"] = mrqa[\"train\"].train_test_split(test_size=0.2)\n",
    "mrqa[\"val\"] = mrqa[\"train\"][\"test\"]\n",
    "mrqa[\"train\"] = mrqa[\"train\"][\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased-distilled-squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 384\n",
    "stride = 128\n",
    "\n",
    "# Preprocessing function for training\n",
    "def preprocess_training_examples(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    answers = examples[\"detected_answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = answers[sample_idx]\n",
    "        start_char = answer[\"char_spans\"][0][\"start\"][0]\n",
    "        end_char = answer[\"char_spans\"][0][\"end\"][0]\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label is (0, 0)\n",
    "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs\n",
    "\n",
    "# Preprocessing function for the validation\n",
    "def preprocess_validation_examples(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    example_ids = []\n",
    "\n",
    "    for i in range(len(inputs[\"input_ids\"])):\n",
    "        sample_idx = sample_map[i]\n",
    "        example_ids.append(examples[\"qid\"][sample_idx])\n",
    "\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        offset = inputs[\"offset_mapping\"][i]\n",
    "        inputs[\"offset_mapping\"][i] = [\n",
    "            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
    "        ]\n",
    "\n",
    "    inputs[\"example_id\"] = example_ids\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8fc3a865b9e498bba9872bcb93e6a6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16537 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac34ae4edac74db294f6de631ff3e779",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5169 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b464a47a28634a579144a7ab1267d49b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4135 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff04d6f94b3a4647913fb602047d50d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5169 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenizing datasets\n",
    "#TODO: Exclude eval set from the preprocess for training examples\n",
    "tokenized_mrqa = mrqa.map(preprocess_training_examples, batched=True, remove_columns=mrqa[\"train\"].column_names)\n",
    "tokenized_mrqa.set_format(type=\"torch\")\n",
    "\n",
    "# Tokenizing evaluation dataset\n",
    "tokenized_eval = mrqa[\"test\"].map(preprocess_validation_examples, batched=True, remove_columns=mrqa[\"test\"].column_names)\n",
    "tokenized_eval.set_format(type=\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the model and testing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining data collator\n",
    "data_collator = DefaultDataCollator()\n",
    "\n",
    "# Configuring parameters for the quantation\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=getattr(torch, \"float16\"),\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "# Configuring parameters of the low-rank adaptation\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=6,\n",
    "    lora_dropout=0.15,\n",
    "    r=2,\n",
    "    bias=\"none\",\n",
    "    task_type=\"QUESTION_ANS\",\n",
    "    target_modules=[\"q_lin\", \"k_lin\", \"v_lin\", \"ffn.lin1\", \"ffn.lin2\", \"attention.out_proj\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading baseline model: DistilBert finetuned on Squadn dataset\n",
    "model = DistilBertForQuestionAnswering.from_pretrained(\"distilbert/distilbert-base-uncased-distilled-squad\",\n",
    "                                                      quantization_config=bnb_config,\n",
    "                                                      device_map={\"\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cleopatra'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the model\n",
    "question, text = \"Who was the last pharaoh of ancient Egypt?\", \"The last pharaoh of ancient Egypt was Cleopatra.\"\n",
    "\n",
    "inputs = tokenizer(question, text, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "answer_start_index = torch.argmax(outputs.start_logits)\n",
    "answer_end_index = torch.argmax(outputs.end_logits)\n",
    "\n",
    "predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
    "tokenizer.decode(predict_answer_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining some parameters for computing the evaluation metrics\n",
    "metric = evaluate.load(\"squad\")\n",
    "n_best = 20\n",
    "max_answer_length = 30\n",
    "\n",
    "# Evaluation funnction with default squad metrics (exact match and f1 score)\n",
    "def compute_metrics(start_logits, end_logits, features, examples):\n",
    "    example_to_features = collections.defaultdict(list)\n",
    "    for idx, feature in enumerate(features):\n",
    "        example_to_features[feature[\"example_id\"]].append(idx)\n",
    "\n",
    "    predicted_answers = []\n",
    "    for example in tqdm(examples):\n",
    "        example_id = example[\"qid\"]\n",
    "        context = example[\"context\"]\n",
    "        answers = []\n",
    "\n",
    "        # Loop through all features associated with that example\n",
    "        for feature_index in example_to_features[example_id]:\n",
    "            start_logit = start_logits[feature_index]\n",
    "            end_logit = end_logits[feature_index]\n",
    "            offsets = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Skip answers that are not fully in the context\n",
    "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                        continue\n",
    "                    # Skip answers with a length that is either < 0 or > max_answer_length\n",
    "                    if (\n",
    "                        end_index < start_index\n",
    "                        or end_index - start_index + 1 > max_answer_length\n",
    "                    ):\n",
    "                        continue\n",
    "\n",
    "                    answer = {\n",
    "                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
    "                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "                    }\n",
    "                    answers.append(answer)\n",
    "\n",
    "        # Select the answer with the best score\n",
    "        if len(answers) > 0:\n",
    "            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "            predicted_answers.append(\n",
    "                {\"id\": example_id, \"prediction_text\": best_answer[\"text\"]}\n",
    "            )\n",
    "        else:\n",
    "            predicted_answers.append({\"id\": example_id, \"prediction_text\": \"\"})\n",
    "\n",
    "    theoretical_answers = [\n",
    "    {\"id\": ex[\"qid\"], \"answers\": [{\"text\": ans, \"answer_start\": 0} for ans in ex[\"answers\"]]}\n",
    "    for ex in examples]\n",
    "    return metric.compute(predictions=predicted_answers, references=theoretical_answers)\n",
    "\n",
    "\n",
    "# Function for doing the complete evaluation on the preprocessed evaluation set\n",
    "def eval_function(tokenized_eval, batch_size=64):\n",
    "    # Remove unneccesary columns from evaluation set and convert it to torch tensors  \n",
    "    eval_set_for_model = tokenized_eval.remove_columns([\"example_id\", \"offset_mapping\"])\n",
    "    eval_set_for_model.set_format(\"torch\")\n",
    "\n",
    "    # Evaluate on GPU if available\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "    # Create a DataLoader for the evaluation set\n",
    "    eval_loader = DataLoader(eval_set_for_model, batch_size=batch_size)\n",
    "\n",
    "    # Log start and end logits\n",
    "    all_start_logits = []\n",
    "    all_end_logits = []\n",
    "\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(eval_loader):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            all_start_logits.append(outputs.start_logits.cpu().numpy())\n",
    "            all_end_logits.append(outputs.end_logits.cpu().numpy())\n",
    "\n",
    "    # Concatennate start and end logit labels\n",
    "    start_logits = np.concatenate(all_start_logits, axis=0)\n",
    "    end_logits = np.concatenate(all_end_logits, axis=0)\n",
    "\n",
    "    return compute_metrics(start_logits, end_logits, tokenized_eval, mrqa[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86224c52a7194ed68251d85dea910372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/82 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05b24b8d4ab04d3fb5544c856a5ce2b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5169 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'exact_match': 83.14954536660863, 'f1': 92.39031321802032}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating evaluation metrics before further finetuning\n",
    "pre_training_metrics = eval_function(tokenized_eval)\n",
    "print(f\"Exact match before finetuning: {pre_training_metrics['exact_match']}\\nF1 score before finetuning: {pre_training_metrics['f1']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the training paramaters and training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\CSANADANSYS\\anaconda3\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:309: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 512\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Defining training parameters \n",
    "output_dir_name = \"trial_run_local\"\n",
    "\n",
    "# Configuring parameters of the low-rank adaptation\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=6,\n",
    "    lora_dropout=0.15,\n",
    "    r=2,\n",
    "    bias=\"none\",\n",
    "    task_type=\"QUESTION_ANS\",\n",
    "    target_modules=[\"q_lin\", \"k_lin\", \"v_lin\", \"ffn.lin1\", \"ffn.lin2\", \"attention.out_proj\"])\n",
    "\n",
    "# Parameters will be later adjusted, this is only to ensure that training pipeline works as intended\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir_name,\n",
    "    eval_strategy=\"steps\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='new_dir',\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_mrqa[\"train\"],\n",
    "    eval_dataset=tokenized_mrqa[\"val\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    peft_config=peft_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "wandb_project = input(\"Wandb project: \")\n",
    "wandb_entity = input(\"Wandb entity: \")\n",
    "wandb.init(project=wandb_project, entity=wandb_entity)\n",
    "trainer.train()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment and run if training was stopped earlier\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating evaluation metrics after further finetuning\n",
    "post_training_metrics = eval_function(tokenized_eval)\n",
    "print(f\"Exact match after finetuning: {post_training_metrics['exact_match']}\\nF1 score after finetuning: {post_training_metrics['f1']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
